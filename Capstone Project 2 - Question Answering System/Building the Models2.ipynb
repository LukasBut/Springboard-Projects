{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Question Answering System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from nltk.tokenize import word_tokenize\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df=pd.read_json(\"C:/Users/Lukas Buteliauskas/Desktop/training_data.json\").reset_index(drop=True)\n",
    "dev_df=pd.read_json(\"C:/Users/Lukas Buteliauskas/Desktop/validation_data.json\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectorization\n",
    "To be able to use words, phrases, questions or other natural language constructs in our model we require a to provide our neural network a numerical representation of our words (as these are the elemental NLP 'particles'). The simplest implementation would be to use 'one hot encoding' and define each word as a vector the size of our dictionary (the number of unique words found in our collection of documents, our corpus). However, this approach will most likely be insufficient for the purposes of a question answering system. word2vec and GloVe are 2 popular choices sophisticated options for word embeddings that also capture word similarities. I will not go into the details of either architecture other than to say that we will not be re-training the word vectors due to the insufficient size of the dataset, and we will begin with the GloVe word embeddings due to it's superior performance in most 'downstream' modelling tasks. Having said that, given the simplicity of swapping word vector representations we will also test out performance with word2vec (providing we can do so in a time-efficient manner).\n",
    "\n",
    "Info and download links for GloVe can be found at: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Custom Functions\n",
    "For the purpose of not repeating code, avoiding bugs and developing good programming practice/design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vector_dict(url_or_path):\n",
    "    \"\"\"Takes a URL or a local path and returns a dictionary of GloVe word vectors where the key is the word and the value is the \n",
    "    word vector with the dimension specified in the input file.\"\"\"\n",
    "    \n",
    "    with open(url_or_path, encoding=\"utf8\") as glove_text:\n",
    "        word_lines=glove_text.readlines()\n",
    "    word_embeddings=[line.split(\" \") for line in word_lines]\n",
    "    word_vector_dict={element[0]:list(map(float, element[1:])) for element in word_embeddings}\n",
    "    \n",
    "    return word_vector_dict\n",
    "\n",
    "\n",
    "def get_word_vector_df(url_path_or_dict):\n",
    "    \"\"\"Takes a URL or path like the previous function, or can take a word vector dictionary and returns a word vector dataframe.\n",
    "    Rows of the dataframe are the word vectors, columns are the dimensions of the word vector, indices are the words.\"\"\"\n",
    "    \n",
    "    if type(url_path_or_dict) is str:\n",
    "        with open(url_path_or_dict, encoding=\"utf8\") as glove_text:\n",
    "            word_lines=glove_text.readlines()\n",
    "        word_embeddings=[line.split(\" \") for line in word_lines]\n",
    "        word_vector_dict={element[0]:list(map(float, element[1:])) for element in word_embeddings}\n",
    "        word_vector_df=pd.DataFrame(word_vector_dict).transpose()\n",
    "    \n",
    "    else:\n",
    "        word_vector_df=pd.DataFrame(url_path_or_dict).transpose()\n",
    "    \n",
    "    return word_vector_df\n",
    "\n",
    "\n",
    "def get_unique_tokens(df):\n",
    "    \"\"\"Given a dataframe containing contexts, questions and answers, the function returns a list of unique tokens.\"\"\"\n",
    "    pieces_of_text=list(df[\"context\"].unique()) + list(df[\"title\"].unique()) + list(df[\"question\"].unique()) \n",
    "    pieces_of_text+=list(df[\"answer_text\"].unique())\n",
    "\n",
    "    non_unique_tokens=[]\n",
    "    for text in pieces_of_text:\n",
    "        temp_tokens=word_tokenize(text)\n",
    "        non_unique_tokens.append(temp_tokens)\n",
    "\n",
    "    unique_tokens=set()\n",
    "    for token in non_unique_tokens:\n",
    "        unique_tokens.update(set(token))\n",
    "    \n",
    "    return [token.replace(\"``\", '\"').replace(\"''\", '\"').lower() for token in list(unique_tokens)]\n",
    "\n",
    "def split_keep_sep(tokens, sep):\n",
    "    \"\"\"Takes a string or a list of tokens, and splits on 'sep' while keeping sep.\"\"\"\n",
    "    split_unique_tokens=[]\n",
    "    for token in tokens:\n",
    "        for sub_token in re.split(\"(\"+ sep + \")\", token):\n",
    "            split_unique_tokens.append(sub_token)\n",
    "    return split_unique_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the Word Vectors\n",
    "As mentioned above with regards to what model we use for the word vectors, it's important to note that the dimention of the word vectors is a hyperparameter of the Neural Networks to come, so to keep our options open we imported a few different word vectors representations and the custom functions defined above make this a 'one line of code' affair (dictionary or dataframe).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_vector_50_dict=get_word_vector_dict(\"C:/Users/Lukas Buteliauskas/Desktop/glove.6B.50d.txt\")\n",
    "word_vector_50_df=get_word_vector_df(word_vector_50_dict)\n",
    "vocab=np.array(word_vector_50_dict.keys()) #400k words as per the documentation.\n",
    "\n",
    "word_vector_100_dict=get_word_vector_dict(\"C:/Users/Lukas Buteliauskas/Desktop/glove.6B.100d.txt\")\n",
    "word_vector_100_df=get_word_vector_df(word_vector_100_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'word_vector_200_dict=get_word_vector_dict(\"C:/Users/Lukas Buteliauskas/Desktop/glove.6B.200d.txt\")\\nword_vector_200_df=get_word_vector_df(word_vector_200_dict)\\n\\nword_vector_300_dict=get_word_vector_dict(\"C:/Users/Lukas Buteliauskas/Desktop/glove.6B.300d.txt\")\\nword_vector_300_df=get_word_vector_df(word_vector_300_dict)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"word_vector_200_dict=get_word_vector_dict(\"C:/Users/Lukas Buteliauskas/Desktop/glove.6B.200d.txt\")\n",
    "word_vector_200_df=get_word_vector_df(word_vector_200_dict)\n",
    "\n",
    "word_vector_300_dict=get_word_vector_dict(\"C:/Users/Lukas Buteliauskas/Desktop/glove.6B.300d.txt\")\n",
    "word_vector_300_df=get_word_vector_df(word_vector_300_dict)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensuring all tokens have word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Main goal is to vectorize stuff into meaningful tokens.\"\"\"\n",
    "\"\"\"We need to deal with the tokens that don't have word embeddings.\n",
    "    1. Deal with numbers\n",
    "    2. Deal with spelling mistakes.\n",
    "    3. Deal with ...\n",
    "    4. Deal with words that just don't have a word embedding representation in GloVe.\n",
    "    \"\"\"\n",
    "unique_tokens=get_unique_tokens(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens with no embedding in GloVe (with the current tokenization):\n",
      "with '-' with words: 31521 \n",
      "with '-' seperate: 25146 \n",
      "\n",
      "['¬£17', 'vergeltungswaffe', 'mar≈°al', '1595‚Äì1610', 'hamaynker', '¬£168.9', 'ozna', 'vented‚Äîa', 'kashua', 'parade‚Äîit', 'estuleks', 'hubway', \"n'namdi\", 'Ààa…™l…ônd/', 'malinoswki', 'znbr', 'drinkinghouse', 'icristat', 'kinseys', 'ÿ®ÿ±ÿßÿØÿ±€å\\u200e', '15408', 'pratyahara', 'majahan', 'paris‚Äìalong', '3,569.8', 'mamillaria', 'acetogenesis', '¬£320', '1up.com', '21‚Äì28', '', '73.9549', 'c.s.c', 'reggane', 'longmenshan', '', 'cohering', 'sichuans', 'annullment', 'bros.h', 'lightolier', 'opsonic', 'bryennius', 'rhinosinusitis', 'relatioship', '110,970', 'stadtstaaten', 'ii‚Äîduring', 'v√©lhop', 'xyndas', 'endownment', 'extremetech', 'tironian', '45‚Äì116', 'prviously', 'chan√©', 'flag‚Äîconsisting', 'cftk', 'proteobacterium', '64cu', 'centers‚Äîthe', \"'almond\", 'chemotrophy', 'castleereagh', 'matais', 'mauveine', 'whiteheadian', '', '1938‚Äì42', 'tukharas', '440s', '‚àí14', '67bn', 'hagkol', '3al', '31250', '20‚Äì22', 'ecumencial', 'ÿß€åÿ±ÿßŸÜ\\u200e\\u200e', 'blaus', 'besan', 'karkota', '1080i/30', 'minyar', '1,322,429', 'settje', 'richmonders', 'narusinsight', 'íäí', 'v√∂sse', 'guardafui', 'vargha', 'bawerk', 'theophrastos', 'investor/philanthropist', '618,271', 'wusizang', 'laemmle/has', 'incluced', 'saarlane']\n"
     ]
    }
   ],
   "source": [
    "no_embeddings=[token for token in unique_tokens if token not in word_vector_50_dict.keys()]\n",
    "split_unique_tokens=split_keep_sep(unique_tokens, \"-\")\n",
    "no_embeddings_new=[token for token in split_unique_tokens if token not in word_vector_50_dict.keys()]\n",
    "print(\"Number of tokens with no embedding in GloVe (with the current tokenization):\")\n",
    "print(\"with '-' with words:\",len(no_embeddings),\"\\nwith '-' seperate:\", len(no_embeddings_new),\"\\n\")\n",
    "print(no_embeddings_new[200:300])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 'Beyonc√© Giselle Knowles-Carter (/biÀêÀàj…ínse…™/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyonc√©\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".'] \n",
      "\n",
      "['Beyonc√©', 'Giselle', 'Knowles-Carter', '(', '/biÀêÀàj…ínse…™/', 'bee-YON-say', ')', '(', 'born', 'September', '4', ',', '1981', ')', 'is', 'an', 'American', 'singer', ',', 'songwriter', ',', 'record', 'producer', 'and', 'actress', '.', 'Born', 'and', 'raised', 'in', 'Houston', ',', 'Texas', ',', 'she', 'performed', 'in', 'various', 'singing', 'and', 'dancing', 'competitions', 'as', 'a', 'child', ',', 'and', 'rose', 'to', 'fame', 'in', 'the', 'late', '1990s', 'as', 'lead', 'singer', 'of', 'R', '&', 'B', 'girl-group', 'Destiny', \"'s\", 'Child', '.', 'Managed', 'by', 'her', 'father', ',', 'Mathew', 'Knowles', ',', 'the', 'group', 'became', 'one', 'of', 'the', 'world', \"'s\", 'best-selling', 'girl', 'groups', 'of', 'all', 'time', '.', 'Their', 'hiatus', 'saw', 'the', 'release', 'of', 'Beyonc√©', \"'s\", 'debut', 'album', ',', 'Dangerously', 'in', 'Love', '(', '2003', ')', ',', 'which', 'established', 'her', 'as', 'a', 'solo', 'artist', 'worldwide', ',', 'earned', 'five', 'Grammy', 'Awards', 'and', 'featured', 'the', 'Billboard', 'Hot', '100', 'number-one', 'singles', '``', 'Crazy', 'in', 'Love', \"''\", 'and', '``', 'Baby', 'Boy', \"''\", '.']\n"
     ]
    }
   ],
   "source": [
    "print(train_df.loc[0,[\"context\"]].values,\"\\n\")\n",
    "first_chap_tokens=word_tokenize(train_df.loc[0,[\"context\"]][0])\n",
    "\n",
    "print(first_chap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
